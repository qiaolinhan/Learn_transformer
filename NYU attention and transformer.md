# [Practicum: Attention and Transformer](https://www.youtube.com/watch?v=f01J0Dri-6k)
Attention (self/ cross; hard/ soft) -- dealing with sets (transformers are maps of sets to sets)
__They do not really deal with Seq2Seq, they can be write as order set, but do not nessessarily need to have order sequences__  
## Self Attention
$$\lbrace x_i \rbrace ^t_{i=1} = \lbrace x_1, x_2, x_3, \cdots, x_t \rbrace,\ \ x_i \in \textrm{R}^n$$
